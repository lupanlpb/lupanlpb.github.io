{"meta":{"title":"Zhuanghua","subtitle":"认清自己","description":"一只USTC菜鸟","author":"Zhuanghua","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2019-05-26T15:07:00.000Z","updated":"2019-05-26T15:07:33.201Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2019-05-26T15:02:27.000Z","updated":"2019-05-26T15:32:20.430Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"关于我男，USTC 固体地球物理学硕士在读，典型菜鸟。"},{"title":"tags","date":"2019-05-26T15:05:51.000Z","updated":"2019-05-26T15:06:14.801Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Logistic Regression","slug":"Logistic-Regression","date":"2019-06-14T04:42:32.000Z","updated":"2019-06-14T05:47:11.483Z","comments":true,"path":"2019/06/14/Logistic-Regression/","link":"","permalink":"http://yoursite.com/2019/06/14/Logistic-Regression/","excerpt":"在分类问题中，要预测的 $y$ 值离散的，逻辑回归是目前最流行使用最广泛的一种用于分类的学习算法。 Hypothesis 分类：$y = 0\\ or\\ 1$ 在逻辑回归中，$0\\ &lt;\\ h_\\theta\\ &lt;1 $， $h_\\theta = g(\\theta^TX)$，其中$X$ 表示特征向量，$g(z)=\\frac{1}{1+e^{-z}}$ 是一个常用的S型$(Sigmoid Function)$ 逻辑函数。","text":"在分类问题中，要预测的 $y$ 值离散的，逻辑回归是目前最流行使用最广泛的一种用于分类的学习算法。 Hypothesis 分类：$y = 0\\ or\\ 1$ 在逻辑回归中，$0\\ &lt;\\ h_\\theta\\ &lt;1 $， $h_\\theta = g(\\theta^TX)$，其中$X$ 表示特征向量，$g(z)=\\frac{1}{1+e^{-z}}$ 是一个常用的S型$(Sigmoid Function)$ 逻辑函数。 12345678910111213141516171819202122import numpy as np# sigmoid functiondef sigmoidFunc(z): return 1.0 / (1.0 + np.exp(-z))# set axesdef originset(): ax = plt.gca() ax.spines['right'].set_color('none') ax.spines['top'].set_color('none') ax.xaxis.set_ticks_position('bottom') ax.yaxis.set_ticks_position('left') ax.spines['bottom'].set_position(('data', 0)) ax.spines['left'].set_position(('data', 0))originset()z = np.arange(-10, 10, 0.1)plt.plot(x, sigmoidFunc(x), 'b')plt.yticks(np.arange(0.2, 1.2, 0.2))plt.title('$Sigmoid\\ Function$') 对$h_\\theta$ 的理解： 在逻辑回归中，当$h_\\theta(x) &gt;= 0.5$ 时，$y = 1$；当$h_\\theta(x) &lt; 0.5$ 时，$y = 0$。 对于一个输入，根据选定的参数计算输出变量为1的可能性，即$h_{\\theta}(x)=P(y=1 | x ; \\theta)$。例如，如果对于给定的$x$，通过已经确定的参数计算得出$h_\\theta(x) = 0.7$，则表示有$70\\%$ 的几率$y$ 为正向类，相应地$y$ 为负向类的几率为$1-0.7=0.3$。 Cost Function 对于线性回归，可以使用模型误差的平方和作为代价函数$J(\\theta)$，在此基础上最小化$J(\\theta)$ 以求得$\\theta $，此时的代价函数是一个凸函数，没有局部最小值，可以很容易的找到全局最小值。但对于逻辑回归模型来说，若按照这样的思路来定义代价函数，此时的代价函数是非凸函数，有就很多的局部极小值，不利于梯度下降法的求解。 于是需要根据逻辑回归的特征重新定义代价函数：$J(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} \\operatorname{cost}\\left(h_{\\theta}\\left(x^{(i)}\\right), y^{(i)}\\right)$。其中$$\\operatorname{cost}\\left(h_{\\theta}(x), y\\right)=\\left{\\begin{aligned}-\\log \\left(h_{\\theta}(x)\\right) &amp; \\text { if } y=1 \\-\\log \\left(1-h_{\\theta}(x)\\right) &amp; \\text { if } y=0 \\end{aligned}\\right.$$ $h_\\theta(x)$ 与$\\operatorname{cost}\\left(h_{\\theta}\\left(x^{(i)}\\right), y^{(i)}\\right)$ 的关系如下图所示： 从上图可以看出$\\operatorname{cost}\\left(h_{\\theta}\\left(x^{(i)}\\right), y^{(i)}\\right)$ 的特点：当$y = 1$时，$h_\\theta$ 越接近$1$，$\\operatorname{cost}\\left(h_{\\theta}\\left(x^{(i)}\\right), y^{(i)}\\right)$ 越小，反之越大；当$y = 0$时，$h_\\theta$ 越接近 $0$，$\\operatorname{cost}\\left(h_{\\theta}\\left(x^{(i)}\\right), y^{(i)}\\right)$ 越小，反之越大。 根据这一特点，构建$\\operatorname{cost}\\left(h_{\\theta}\\left(x^{(i)}\\right), y^{(i)}\\right)$ 如下：$\\operatorname{cost}\\left(h_{\\theta}(x), y\\right)=-y \\times \\log \\left(h_{\\theta}(x)\\right)-(1-y) \\times \\log \\left(1-h_{\\theta}(x)\\right)$。 据此可得代价函数：$J(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m}\\left[-y^{(i)} \\log \\left(h_{\\theta}\\left(x^{(i)}\\right)\\right)-\\left(1-y^{(i)}\\right) \\log \\left(1-h_{\\theta}\\left(x^{(i)}\\right)\\right)\\right]$。 123456789# cost functiondef costfn(theta, X, y, mylambda = 0): term1 = np.dot(-y.T, np.log(h(theta, X))) term2 = np.dot((1 - y).T, np.log(1 - h(theta, X)))# regularized termregn = (mylambda/ (2.0 * m)) * np.sum(np.power(theta[1:], 2))return float(1.0/m * np.sum(term1 - term2) + regn) Gradient在得到代价函数之后，我们便可以利用梯度下降法来求解使代价函数最小的参数$\\theta$。 求解算法：$$\\begin{array}{c}{\\text { Repeat }\\left{\\theta_{j} :=\\theta_{j}-\\alpha \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)\\right.} \\ {\\text { (simultaneously update all } \\theta_{j} )} \\ {\\mathrm{~ } ~}}\\end{array}$$ 求导后带入可得：$$\\begin{array}{l}{\\text { Repeat }\\left{\\theta_{j} :=\\theta_{j}-\\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(\\mathrm{x}^{(i)}\\right)-\\mathrm{y}^{(i)} \\quad\\right) \\mathrm{x}{j}^{(i)}\\right.} \\ {\\text { (simultaneously update all } \\theta{j} )} \\ {\\mathrm{~ } ~}}\\end{array}$$ Summary主要公式： Sigmoid function: $g(z)=\\frac{1}{1+e^{-z}}$ Hypothesis: $h_\\theta = g(\\theta^TX) = g(z)=\\frac{1}{1+e^{-\\theta^TX}}$ $\\operatorname{cost}:$ $\\operatorname{cost}\\left(h_{\\theta}(x), y\\right)=-y \\times \\log \\left(h_{\\theta}(x)\\right)-(1-y) \\times \\log \\left(1-h_{\\theta}(x)\\right)$ Cost function: $J(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m}\\left[-y^{(i)} \\log \\left(h_{\\theta}\\left(x^{(i)}\\right)\\right)-\\left(1-y^{(i)}\\right) \\log \\left(1-h_{\\theta}\\left(x^{(i)}\\right)\\right)\\right]$ Gradient: $\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}=\\frac{1}{m} \\sum_{i=1}^{m}\\left[h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right] x_{j}^{(i)}$ Appendix：the deviation of $J (\\theta)$$$J(\\theta)=-\\frac{1}{m} \\sum_{i=1}^{m}\\left[y^{(i)} \\log \\left(h_{\\theta}\\left(x^{(i)}\\right)\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-h_{\\theta}\\left(x^{(i)}\\right)\\right)\\right]$$ 由于$h_{\\theta}\\left(x^{(i)}\\right)=\\frac{1}{1+e^{-\\theta^{T} x^{(i)}}}$，则：$$\\begin{array}{l}{y^{(i)} \\log \\left(h_{\\theta}\\left(x^{(i)}\\right)\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-h_{\\theta}\\left(x^{(i)}\\right)\\right)} \\\\ {=y^{(i)} \\log \\left(\\frac{1}{1+e^{-\\theta^{T} x^{(i)}}}\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-\\frac{1}{1+e^{-\\theta^{T} x^{(i)}}}\\right)} \\\\ {=-y^{(i)} \\log \\left(1+e^{-\\theta^{T} x^{(i)}}\\right)-\\left(1-y^{(i)}\\right) \\log \\left(1+e^{\\theta^{T} x^{(i)}}\\right)}\\end{array}$$ 所以，$$\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)=\\frac{\\partial}{\\partial \\theta_{j}}\\left[-\\frac{1}{m} \\sum_{i=1}^{m}\\left[-y^{(i)} \\log \\left(1+e^{-\\theta^{T} x^{(i)}}\\right)-\\left(1-y^{(i)}\\right) \\log \\left(1+e^{\\theta^{T} x^{(i)}}\\right)\\right]\\right]$$ $$\\begin{array}{l}{=-\\frac{1}{m} \\sum_{i=1}^{m}\\left[-y^{(i)} \\frac{-x_{j}^{(i)} e^{-\\theta^{T} x^{(i)}}}{1+e^{-\\theta^{T} x^{(i)}}}-\\left(1-y^{(i)}\\right) \\frac{x_{j}^{(i)} e^{\\theta^{T} x^{(i)}}}{1+e^{\\theta^{T} x^{(i)}}}\\right]} \\\\ {=-\\frac{1}{m} \\sum_{i=1}^{m} y^{(i)} \\frac{x_{j}^{(i)}}{1+e^{\\theta^{T} x^{(i)}}}-\\left(1-y^{(i)}\\right) \\frac{x_{j}^{(i)} e^{\\theta^{T} x^{(i)}}}{1+e^{\\theta^{T} x^{(i)}}} ]} \\\\ {=-\\frac{1}{m} \\sum_{i=1}^{m} \\frac{y^{(i)} x_{j}^{(i)}-x_{j}^{(i)} e^{\\theta^{T} x^{(i)}}+y^{(i)} x_{j}^{(i)} e^{\\theta^{T} x^{(i)}}}{1+e^{\\theta^{T} x^{(i)}}}} \\\\ {=-\\frac{1}{m} \\sum_{i=1}^{m} \\frac{y^{(i)}\\left(1+e^{\\theta^{T} x^{(i)}}\\right)-e^{\\theta^{T} x^{(i)}}}{1+e^{\\theta^{T} x^{(i)}}} x_{j}^{(i)}} \\\\ {=-\\frac{1}{m} \\sum_{i=1}^{m}\\left(y^{(i)}-\\frac{e^{\\theta^{T} x^{(i)}}}{1+e^{\\theta^{T} x^{(i)}}}\\right) x_{j}^{(i)}} \\\\ {=-\\frac{1}{m} \\sum_{i=1}^{m}\\left(y^{(i)}-\\frac{1}{1+e^{-\\theta^{T} x^{(i)}} )} x_{j}^{(i)}\\right.} \\\\ {=-\\frac{1}{m} \\sum_{i=1}^{m}\\left[y^{(i)}-h_{\\theta}\\left(x^{(i)}\\right)\\right] x_{j}^{(i)}} \\\\ {=\\frac{1}{m} \\sum_{i=1}^{m}\\left[h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right] x_{j}^{(i)}}\\end{array}$$即$$\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}=\\frac{1}{m} \\sum_{i=1}^{m}\\left[h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right] x_{j}^{(i)}$$","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"},{"name":"逻辑回归","slug":"逻辑回归","permalink":"http://yoursite.com/tags/逻辑回归/"},{"name":"Sigmoid Function","slug":"Sigmoid-Function","permalink":"http://yoursite.com/tags/Sigmoid-Function/"},{"name":"梯度下降法","slug":"梯度下降法","permalink":"http://yoursite.com/tags/梯度下降法/"}]},{"title":"远程工具的使用","slug":"远程工具的使用","date":"2019-05-31T14:46:48.000Z","updated":"2019-05-31T14:47:20.371Z","comments":true,"path":"2019/05/31/远程工具的使用/","link":"","permalink":"http://yoursite.com/2019/05/31/远程工具的使用/","excerpt":"ssh：在同一局域网下，远程连接 安装openssh-server 1sudo apt-get install openssh-server # 安装openssh-server 远程访问","text":"ssh：在同一局域网下，远程连接 安装openssh-server 1sudo apt-get install openssh-server # 安装openssh-server 远程访问 1ifconfig # 查看远程linux系统的IP地址 在本地终端输入如下代码： 1ssh username@IP地址 进行访问 请求访问的时候会要求输入远程用户密码 vnc：在同一局域网下远程共享桌面 linux安装x11vnc 1sudo apt-get install x11vnc # 安装vnc 设置密码 1x11vnc -storepasswd # 设置密码 终端执行以下代码，进行远程共享桌面 1x11vnc -usepw # 启动远程共享桌面 samba：在同一局域网下共享文件 在远程文件目录下新建文件夹，命名为Share,然后右键选中选择”Local Network Share” 在跳出的页面中勾选Share this folder,并勾选”Allow others to creat and delete files in this folder” 在终端输入以下命令,设置密码 1sudo smbpasswd -a username # 设置密码 在Mac中按下command + 跳出如下选框 在框内输入 1smb://远程linux的IP地址 然后输入用户名及密码，进行访问。","categories":[],"tags":[{"name":"ssh","slug":"ssh","permalink":"http://yoursite.com/tags/ssh/"},{"name":"vnc","slug":"vnc","permalink":"http://yoursite.com/tags/vnc/"},{"name":"smb","slug":"smb","permalink":"http://yoursite.com/tags/smb/"},{"name":"远程工具","slug":"远程工具","permalink":"http://yoursite.com/tags/远程工具/"}]},{"title":"git学习笔记","slug":"git学习笔记","date":"2019-05-31T14:45:51.000Z","updated":"2019-05-31T14:46:24.296Z","comments":true,"path":"2019/05/31/git学习笔记/","link":"","permalink":"http://yoursite.com/2019/05/31/git学习笔记/","excerpt":"Git的三个工作区域 工作区 暂存区 Git仓库","text":"Git的三个工作区域 工作区 暂存区 Git仓库 Git提交文件常用命令12345678git config --list # 查看配置列表git config --global user.email 'lupanlpb@163.com' # 配置邮箱git config --global user.name 'lupanlpb' # 配置用户名git clone https://github.com/lupanlpb/GitLearn.git # 克隆远程仓库到本地git status # 查看状态git add test.dat # 将文件从工作区提交到暂存区git commit -m \"提交的描述\" # 将文件从暂存区提交到Git仓库git push # 提交到远程仓库","categories":[],"tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}]},{"title":"Mac下利用Hexo搭建个人博客","slug":"Mac下利用Hexo搭建个人博客","date":"2019-05-31T14:44:44.000Z","updated":"2019-05-31T14:45:24.223Z","comments":true,"path":"2019/05/31/Mac下利用Hexo搭建个人博客/","link":"","permalink":"http://yoursite.com/2019/05/31/Mac下利用Hexo搭建个人博客/","excerpt":"1. 安装git2. 安装node.js3. 安装cnpm","text":"1. 安装git2. 安装node.js3. 安装cnpm 1234sudo su # 切换到root用户npm -v # 查看node.js是否安装成功npm install -g cnpm --registry=https://registry.npm.taobao.org #安装镜像cnpm -v # 查看安装cnpm是否成功 4. 安装hexo12cnpm install -g hexo-cli # 安装hexohexo -v # 查看是否安装成功 5. 创建博客12345678910111213mkdir blog # 博客目录cd blog # 切换到blog目录sudo hexo init # 初始化博客hexo s #启动博客，在浏览器输入localhost:4000hexo n \"My first blog\" # 新建一篇博客文章cd source/_posts/ # 切换目录vim My-first-blog.md # 编辑博客cd ../../ # 切换到blog目录hexo clean # 清理hexo g # 生成博客hexo s # 启动# 到目前为止，博客已经搭建完毕# 下面将博客部署到github 6. 部署博客到github1234567891011121314151617# 1. 登录github# 2. 新建仓库# 3. 在repository name输入你的github昵称 + github.io, 如\"lupanlpb.github.io”# 4. 在description输入介绍信息# 5. 点击create# 6. 安装git部署插件cnpm install --save hexo-deployer-git # 安装过程会报出warning，可不予理会# 7. 修改配置文件vim _config.yml ## 8. 在 Deployment处修改type、reporepo和branch如下deploy: type: git repo: https://github.com/lupanlpb/lupanlpb.github.io.git branch: master# 保存退出# 9. 部署到远端hexo d # 部署，并根据提示输入github账号和密码 现在可以访问https://lupanlpb.github.io/了，这样博客就部署到github了！ 7. 更换主题 推荐使用Even123456git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia # 下载主题，在blog/themes目录下vim _config.yml # 修改配置文件将theme: landscape改为theme: yilia,保存退出hexo cleanhexo ghexo shexo d # 部署到远端","categories":[],"tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"},{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"Mac","slug":"Mac","permalink":"http://yoursite.com/tags/Mac/"},{"name":"github","slug":"github","permalink":"http://yoursite.com/tags/github/"},{"name":"Even","slug":"Even","permalink":"http://yoursite.com/tags/Even/"},{"name":"个人博客","slug":"个人博客","permalink":"http://yoursite.com/tags/个人博客/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-05-26T15:37:19.240Z","updated":"2019-05-26T15:37:19.240Z","comments":true,"path":"2019/05/26/hello-world/","link":"","permalink":"http://yoursite.com/2019/05/26/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post 1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"}]},{"title":"My First Blog","slug":"My-First-Blog","date":"2019-05-26T09:26:46.000Z","updated":"2019-05-26T15:38:40.554Z","comments":true,"path":"2019/05/26/My-First-Blog/","link":"","permalink":"http://yoursite.com/2019/05/26/My-First-Blog/","excerpt":"","text":"This is my first blog powered by HexoThat’s Great!","categories":[],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"Great!","slug":"Great","permalink":"http://yoursite.com/tags/Great/"}]}]}